model: 't5-base'
output_dir: 't5_results'
batch_size: 16
epochs: 5
max_token_length: 64
evaluation_strategy: 'epoch'
per_device_train_batch_size: 16
per_device_eval_batch_size: 16
learning_rate: 0.00002
weight_decay: 0.01
save_strategy: 'epoch'
predict_with_generate: True
fp16: True
gradient_accumulation_steps: 6
eval_steps: 500
save_steps: 500
load_best_model_at_end: True
logging_dir: "t5_results/logs"
report_to: "wandb"
